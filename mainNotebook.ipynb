{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf55d7-6961-4e77-92aa-4b0d977680b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Pull AWS access keys from environment variables previously set \n",
    "# in Jupyter Hub\n",
    "# =============================================================\n",
    "key_id          = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_key      = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "session         = boto3.session.Session(aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "\n",
    "s3_client       = boto3.client('s3',\n",
    "                  aws_access_key_id=key_id,\n",
    "                  aws_secret_access_key=secret_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9afd04-f54f-4e2d-8605-3f9aa54dbb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Download jobs file from S3 and place it into a dataframe (df)\n",
    "#===============================================================\n",
    "bucket_name     = 'rhods-pilot'\n",
    "file_name       = 'LJSpeech-1.1'\n",
    "new_file_name   = 'audio_data'\n",
    "local_dest_dir  = os.path.join(os.getcwd(), 'datasets')\n",
    "\n",
    "s3_client.download_file(bucket_name, file_name, new_file_name)\n",
    "\n",
    "#place file contents into a dataframe for processing\n",
    "df              = pd.read_csv(new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb3136-0f67-4d71-a6dd-b0b4995dc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatter\n",
    "\n",
    "def ljspeech(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n",
    "    \"\"\"Normalizes the LJSpeech meta data file to TTS format\n",
    "    https://keithito.com/LJ-Speech-Dataset/\"\"\"\n",
    "    txt_file = os.path.join(root_path, meta_file)\n",
    "    items = []\n",
    "    speaker_name = \"ljspeech\"\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n",
    "        for line in ttf:\n",
    "            cols = line.split(\"|\")\n",
    "            wav_file = os.path.join(root_path, \"wavs\", cols[0] + \".wav\")\n",
    "            text = cols[2]\n",
    "            items.append({\"text\": text, \"audio_file\": wav_file, \"speaker_name\": speaker_name})\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3dbdf-f1d0-4fcd-a7d1-de924f4550ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data = ljspeech(\"/datasets/\", 'metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114fc62-6bcc-41d2-95f0-ebf28f32de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Trainer: Where the ‚ú®Ô∏è happens.\n",
    "# TrainingArgs: Defines the set of arguments of the Trainer.\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "# GlowTTSConfig: all model related values for training, validating and testing.\n",
    "from configs.glow_tts_config import GlowTTSConfig\n",
    "\n",
    "# BaseDatasetConfig: defines name, formatter and path of the dataset.\n",
    "from configs.shared_configs import BaseDatasetConfig\n",
    "from datasets import load_tts_samples\n",
    "from glow_tts import GlowTTS\n",
    "from utils.text.tokenizer import TTSTokenizer\n",
    "from utils.audio import AudioProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc596c-b237-4d43-96ca-b96121b78029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the same path as this script as our training folder.\n",
    "output_path = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# DEFINE DATASET CONFIG\n",
    "# Set LJSpeech as our target dataset and define its path.\n",
    "# You can also use a simple Dict to define the dataset and pass it to your custom formatter.\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    name=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../datasets/LJSpeech-1.1/\")\n",
    ")\n",
    "\n",
    "# INITIALIZE THE TRAINING CONFIGURATION\n",
    "# Configure the model. Every config class inherits the BaseTTSConfig.\n",
    "config = GlowTTSConfig(\n",
    "    batch_size=32,\n",
    "    eval_batch_size=16,\n",
    "    num_loader_workers=4,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    epochs=10,\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    use_phonemes=True,\n",
    "    phoneme_language=\"en-us\",\n",
    "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
    "    print_step=5,\n",
    "    print_eval=False,\n",
    "    mixed_precision=True,\n",
    "    output_path=output_path,\n",
    "    datasets=[dataset_config],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4a730-4c61-4908-9604-af33aa640a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE THE AUDIO PROCESSOR\n",
    "# Audio processor is used for feature extraction and audio I/O.\n",
    "# It mainly serves to the dataloader and the training loggers.\n",
    "ap = AudioProcessor.init_from_config(config)\n",
    "\n",
    "# INITIALIZE THE TOKENIZER\n",
    "# Tokenizer is used to convert text to sequences of token IDs.\n",
    "# If characters are not defined in the config, default characters are passed to the config\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "\n",
    "# LOAD DATA SAMPLES\n",
    "# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n",
    "# We use our defined format function for the Ljspeech dataset\n",
    "train_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True, formatter=ljspeech)\n",
    "\n",
    "# INITIALIZE THE MODEL\n",
    "# Models take a config object and a speaker manager as input\n",
    "# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n",
    "# Speaker manager is used by multi-speaker models.\n",
    "model = GlowTTS(config, ap, tokenizer, speaker_manager=None)\n",
    "\n",
    "# INITIALIZE THE TRAINER\n",
    "# Trainer provides a generic API to train all the üê∏TTS models with all its perks like mixed-precision training,\n",
    "# distributed training, etc.\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n",
    ")\n",
    "\n",
    "# AND... 3,2,1... üöÄ\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b97f5-9c2b-4c5d-87cb-99b7704d1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tts --text \"My sprinkler goes like this ststststststststststst and comes back like ttttttttttttttttttttttttttttttttttttttte\" \\\n",
    "--model_name \"tts_models/en/ljspeech/glow-tts\" \\\n",
    "--vocoder_name \"vocoder_models/universal/libri-tts/fullband-melgan\" \\\n",
    "--out_path tts_output.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f2553b-ca79-4c30-86bb-d5d32e81bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(filename='tts_output.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58352a-0397-4bfd-a42d-cf43eece2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocoder trainer if we want to include it\n",
    "\n",
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.vocoder.configs import HifiganConfig\n",
    "from TTS.vocoder.datasets.preprocess import load_wav_data\n",
    "from TTS.vocoder.models.gan import GAN\n",
    "\n",
    "output_path = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "config = HifiganConfig(\n",
    "    batch_size=32,\n",
    "    eval_batch_size=16,\n",
    "    num_loader_workers=4,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=5,\n",
    "    epochs=1000,\n",
    "    seq_len=8192,\n",
    "    pad_short=2000,\n",
    "    use_noise_augment=True,\n",
    "    eval_split_size=10,\n",
    "    print_step=25,\n",
    "    print_eval=False,\n",
    "    mixed_precision=False,\n",
    "    lr_gen=1e-4,\n",
    "    lr_disc=1e-4,\n",
    "    data_path=os.path.join(output_path, \"../LJSpeech-1.1/wavs/\"),\n",
    "    output_path=output_path,\n",
    ")\n",
    "\n",
    "# init audio processor\n",
    "ap = AudioProcessor(**config.audio.to_dict())\n",
    "\n",
    "# load training samples\n",
    "eval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n",
    "\n",
    "# init model\n",
    "model = GAN(config)\n",
    "\n",
    "# init the trainer and üöÄ\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(),\n",
    "    config,\n",
    "    output_path,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    "    training_assets={\"audio_processor\": ap},\n",
    ")\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
